reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
if(ncon != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Control")
for(j in 1:ncon){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
}
View(reshapeEsoph)
esoph
gm1 <- glm(status ~ ., data=reshapeEsoph)
summary(gm1)
rse <- reshapeEsoph
rse$status <- as.character(rse$status)
gm1 <- glm(status ~ ., data=rse)
gm1 <- glm(status ~ ., data=rse, family="binomial")
gm1 <- glm(status ~ ., data=reshapeEsoph, family="binomial")
summary(gm1)
gm1 <- glm(status ~ ., data=reshapeEsoph, family="binomial")
summary(gm1)
intModel <- glm(status ~ 1, data=reshapeEsoph, family="binomial")
summary(intModel)
gm1 <- glm(status ~ ., data=reshapeEsoph, family="binomial")
summary(gm1)
pchisq(1072-898.86, df=1174-1163, lower=F)
library(tidyverse)  ## basic data manipulation and plotting package in R (e.g. dplyr, ggplot2)
library(caret)      ## cross-validation and modeling tools
###
### 2) Linear regression of infant mortality in `swiss` dataset
###    Just building models and showing syntax: No diagnostics here...
###
### 2i) Simple linear regression
m1 <- lm(Infant.Mortality ~ Fertility, data=swiss)
summary(m1)
### 2ii) Multiple linear regression (using all predictors)
m2 <- lm(Infant.Mortality ~ ., data=swiss)
summary(m2)
### 2iii) Same as above but add squared predictors
m3 <- lm(Infant.Mortality ~ Fertility + Agriculture + Examination + Education + Catholic +
I(Fertility^2) + I(Agriculture^2) + I(Examination^2) + I(Education^2) + I(Catholic^2),
data=swiss)
summary(m3)
###
### 3) Do simple variable selection in `mtcars` dataset; model `mpg`
###
### 3i) Note presense of high correlation -- signifies need for variable selection
mtcars %>% cor %>% round(2)
### 3ii) Further evidence of co-linearity problems in modeling
###        First, note many positive trends with `mpg`
plot(mtcars)
###        Now, note that no terms reach significance in full model -- co-linearity is possible problem
m4 <- lm(mpg ~ ., data=mtcars)
summary(m4)
### 3iii) Split into test and training data sets; 30/70 split
set.seed(123)
n         <- nrow(mtcars)
p         <- ncol(mtcars)
trainI    <- sample(1:n, size=0.7*n, replace=F)
trainData <- mtcars[trainI,]
testData  <- mtcars[-trainI,]
### 3iv) Set training control (10-fold cross-validation with 5 repeats), run var selection
###      Variable selection: stepwise (forward and backward) using AIC selection
myControl <- trainControl(method="repeatedcv", number=10, repeats=5)
lmFit     <- train(mpg ~ .,
data=trainData,
trControl=myControl,
method="lmStepAIC",
trace=F)
summary(lmFit)
### 3v) estimate R-squared with test dataset
predictMPG     <- predict(lmFit, newdata=select(testData, -mpg))
estCorrelation <- cor(predictMPG, testData$mpg)
estR2          <- estCorrelation^2
estR2
###
### 4) Logistic regression example, using esophogeal cancer data after reshaping
###
### 4i) Quick look at the data, then reshape data for logistic regression
###     Reshape to a tidy data set -- 1 row = 1 case or 1 control
head(esoph)
reshapeEsoph <- esoph[0,] %>%
select(-ncases, -ncontrols) %>%
mutate(status = factor(character(), levels=c("Control", "Case")))
for(i in 1:nrow(esoph)){
agegp <- esoph[i, 1]
alcgp <- esoph[i, 2]
tobgp <- esoph[i, 3]
ncas  <- esoph[i, 4]
ncon  <- esoph[i, 5]
if(ncas != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Case")
print(temp)
for(j in 1:ncas){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
if(ncon != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Control")
for(j in 1:ncon){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
}
### 4ii) Perform logistic regression on reshaped ata
gm1 <- glm(status ~ ., data=reshapeEsoph, family="binomial")
summary(gm1)
for(i in 1:nrow(esoph)){
agegp <- esoph[i, 1]
alcgp <- esoph[i, 2]
tobgp <- esoph[i, 3]
ncas  <- esoph[i, 4]
ncon  <- esoph[i, 5]
if(ncas != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Case")
for(j in 1:ncas){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
if(ncon != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Control")
for(j in 1:ncon){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
}
reshapeEsoph <- esoph[0,] %>%
select(-ncases, -ncontrols) %>%
mutate(status = factor(character(), levels=c("Control", "Case")))
for(i in 1:nrow(esoph)){
agegp <- esoph[i, 1]
alcgp <- esoph[i, 2]
tobgp <- esoph[i, 3]
ncas  <- esoph[i, 4]
ncon  <- esoph[i, 5]
if(ncas != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Case")
for(j in 1:ncas){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
if(ncon != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Control")
for(j in 1:ncon){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
}
View(reshapeEsoph)
gm1 <- glm(status ~ ., data=reshapeEsoph, family="binomial")
summary(gm1)
data <- read_csv("train.csv")
library(tidyverse)  # for data manipulation (dplyr) tools and ggplot2
data <- read_csv("train.csv")
head(data)
sapply(data, FUN = function(x) sum(is.na(x)))
data <- select(critical_temp, everything())
data <- select(data, critical_temp, everything())
head(data)
stdData <- sapply(depVar, function(x) scale(x, center=T, scale=T))
depVar  <- data[,-1]
stdData <- sapply(depVar, function(x) scale(x, center=T, scale=T))
sapply(stdData, mean)
dim(stdData)
sapply(stdData, function(x) mean(x))
colMeans(stdData)
library(matrixStats_)
library(matrixStats)
colSds(stdData)
pcadata <- prcomp(stdData)
summary(pcaData)
summary(pcadata)
rm(pcadata)
pcaData <- prcomp(stdData)
attributes(pcaData)
cumVar <- cumsum(pcaData$sdev^2)/sum(pcadata$sdev^2)
cumVar <- cumsum(pcaData$sdev^2)/sum(pcaData$sdev^2)
cumVar
which(cumVar > 0.95)
which(cumVar > 0.95) %>% min
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component")
abline(h = critPC, color = "red")
cumVar <- cumsum(pcaData$sdev^2)/sum(pcaData$sdev^2)
critPC <- which(cum > 0.95) %>% min
critPC
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component",
type = "b")
abline(h = critPC, color = "red")
critPC <- which(cumVar > 0.95) %>% min
critPC
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component",
type = "b")
abline(h = critPC, color = "red")
abline(h = critPC, col = "red")
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component",
type = "b")
abline(v = critPC, col = "red")
text(paste0("Min PC # for 95% Cumulative Var: ", critPC))
text(paste0("Min PC # for 95% Cumulative Var: ", critPC), x=critPC + 50)
text(paste0("Min PC # for 95% Cumulative Var: ", critPC), x=critPC)
text(paste0("Min PC # for 95% Cumulative Var: ", critPC), x=50)
cumVar <- cumsum(pcaData$sdev^2)/sum(pcaData$sdev^2)
critPC <- which(cumVar > 0.95) %>% min
critPC
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component",
type = "b")
abline(v = critPC, col = "red")
text(paste0("Min PC # for 95% Cumulative Var: ", critPC), x=50)
text(paste0("Min PC # for 95% Cumulative Var: ", critPC), pos=3)
text(x=25, y=-0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), pos=3)
text(x=5 y=-0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), pos=3)
text(x=5, y=-0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), pos=3)
text(x=0.5, y=0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), col="red")
text(x=critPC+3, y=0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), col="red")
text(x=critPC+15, y=0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), col="red")
text(x=critPC+20, y=0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), col="red")
cumVar <- cumsum(pcaData$sdev^2)/sum(pcaData$sdev^2)
critPC <- which(cumVar > 0.95) %>% min
critPC
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component",
type = "b")
abline(v = critPC, col = "red")
text(x=critPC+20, y=0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), col="red")
dim(pcaData$x)
library(caret)
n         <- nrow(critData)
trainI    <- sample(1:n, size=0.8*n, replace=F)
trainData <- critData[trainI,]
testData  <- critData[-trainI,]
myControl <- trainControl(method="repeatedcv", number=10)
rfFit     <- train(critical_temp ~ .,
data=trainData,
trContro=myControl,
method="rf")
### select critical PC data
critData <- pcaData$x[,1:critPC]
###
### 4) model with data post-dimensional reduction data, using random forest, summarize
###    cross validate with 10-fold cross-validation
###    note: just using default tuning grid for hyperparameter selection
###
### separate into training and test subsets
n         <- nrow(critData)
trainI    <- sample(1:n, size=0.8*n, replace=F)
trainData <- critData[trainI,]
testData  <- critData[-trainI,]
myControl <- trainControl(method="repeatedcv", number=10)
rfFit     <- train(critical_temp ~ .,
data=trainData,
trContro=myControl,
method="rf")
critData <- pcaData$x[,1:critPC] %>%
mutate(critical_temp = data$critical_temp) %>%
select(critical_temp, everything())
critData <- pcaData$x[,1:critPC] %>%
mutate(critical_temp = data$critical_temp) %>%
select(critical_temp, everything())
data.frame(a=c(1,2,3)) %>% mutate(b=c(4,5,6))
data$critical_temp
critData               <- pcaData$x[,1:critPC]
critData$critical_temp <- data$critical_temp
critData               <- select(critdata, critical_temp, everything())
critData               <- pcaData$x[,1:critPC]
critData$critical_temp <- data$critical_temp
critData               <- select(critData, critical_temp, everything())
library(tidyverse)
critData               <- pcaData$x[,1:critPC]
class(critDtaa)
class(critData)
critData %>% as.data.frame
critData               <- pcaData$x[,1:critPC] %>%
as.data.frame %>%
mutate(critical_temp = data$critical_temp) %>%
select(critical_temp, everything())
critData
myControl <- trainControl(method="repeatedcv", number=10)
rfFit     <- train(critical_temp ~ .,
data=trainData,
trContro=myControl,
method="rf")
n         <- nrow(critData)
trainI    <- sample(1:n, size=0.8*n, replace=F)
trainData <- critData[trainI,]
testData  <- critData[-trainI,]
myControl <- trainControl(method="repeatedcv", number=10)
rfFit     <- train(critical_temp ~ .,
data=trainData,
trContro=myControl,
method="rf")
n         <- nrow(critData)
trainI    <- sample(1:n, size=0.8*n, replace=F)
trainData <- critData[trainI,]
testData  <- critData[-trainI,]
### perform random forest modeling
myControl <- trainControl(method="none")
rfFit     <- train(critical_temp ~ .,
data=trainData,
trControl=myControl,
method="rf")
myControl <- trainControl(method="none")
rfFit     <- train(critical_temp ~ .,
data=trainData,
trControl=myControl,
method="rf")
set.seed(123)
n         <- nrow(critData)
trainI    <- sample(1:n, size=0.8*n, replace=F)
trainData <- critData[trainI,]
testData  <- critData[-trainI,]
### perform random forest modeling
myControl <- trainControl(method="none")
rfFit     <- train(critical_temp ~ .,
data=trainData,
trControl=myControl,
method="rf")
### save `rfFit` so it doesn't have to be re-calculated
save(rfFit, file="rfFit.robject")
load("rfFit.robject")
# Title:   Coding Example - General and Generalized Linear Regression in R
# Author:  Peter Scarbrough
# Date:    6 Jan 2020
# Purpose: Demonstrate how to build and examine some basic LR and GRL
#          Models in R
###
### 1) Load required packages
###
library(tidyverse)  ## basic data manipulation and plotting package in R (e.g. dplyr, ggplot2)
library(caret)      ## cross-validation and modeling tools
###
### 2) Linear regression of infant mortality in `swiss` dataset
###    Just building models and showing syntax: No diagnostics here...
###
### 2i) Simple linear regression
m1 <- lm(Infant.Mortality ~ Fertility, data=swiss)
summary(m1)
### 2ii) Multiple linear regression (using all predictors)
m2 <- lm(Infant.Mortality ~ ., data=swiss)
summary(m2)
### 2iii) Same as above but add squared predictors
m3 <- lm(Infant.Mortality ~ Fertility + Agriculture + Examination + Education + Catholic +
I(Fertility^2) + I(Agriculture^2) + I(Examination^2) + I(Education^2) + I(Catholic^2),
data=swiss)
summary(m3)
###
### 3) Do simple variable selection in `mtcars` dataset; model `mpg`
###
### 3i) Note presense of high correlation -- signifies need for variable selection
mtcars %>% cor %>% round(2)
### 3ii) Further evidence of co-linearity problems in modeling
###        First, note many positive trends with `mpg`
plot(mtcars)
###        Now, note that no terms reach significance in full model -- co-linearity is possible problem
m4 <- lm(mpg ~ ., data=mtcars)
summary(m4)
### 3iii) Split into test and training data sets; 30/70 split
set.seed(123)
n         <- nrow(mtcars)
p         <- ncol(mtcars)
trainI    <- sample(1:n, size=0.7*n, replace=F)
trainData <- mtcars[trainI,]
testData  <- mtcars[-trainI,]
### 3iv) Set training control (10-fold cross-validation with 5 repeats), run var selection
###      Variable selection: stepwise (forward and backward) using AIC selection
myControl <- trainControl(method="repeatedcv", number=10, repeats=5)
lmFit     <- train(mpg ~ .,
data=trainData,
trControl=myControl,
method="lmStepAIC",
trace=F)
summary(lmFit)
### 3v) estimate R-squared with test dataset
predictMPG     <- predict(lmFit, newdata=select(testData, -mpg))
estCorrelation <- cor(predictMPG, testData$mpg)
estR2          <- estCorrelation^2
estR2
###
### 4) Logistic regression example, using esophogeal cancer data after reshaping
###
### 4i) Quick look at the data, then reshape data for logistic regression
###     Reshape to a tidy data set -- 1 row = 1 case or 1 control
### notice starting data structure
head(esoph)
### create empty data frame to hold reshaped result
reshapeEsoph <- esoph[0,] %>%
select(-ncases, -ncontrols) %>%
mutate(status = factor(character(), levels=c("Control", "Case")))
### loop through original data set and reshape to tidy data frame
for(i in 1:nrow(esoph)){
agegp <- esoph[i, 1]
alcgp <- esoph[i, 2]
tobgp <- esoph[i, 3]
ncas  <- esoph[i, 4]
ncon  <- esoph[i, 5]
if(ncas != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Case")
for(j in 1:ncas){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
if(ncon != 0){
temp  <- data.frame(agegp=agegp, alcgp=alcgp, tobgp=tobgp, status="Control")
for(j in 1:ncon){
reshapeEsoph <- rbind(reshapeEsoph, temp)
}
}
}
### 4ii) Perform logistic regression on reshaped data
###      Just calculating the full model -- no diagnostics or features selection
gm1 <- glm(status ~ ., data=reshapeEsoph, family="binomial")
summary(gm1)
# Title:   Coding Example - Demonstration of Principal Component Analysis
# Author:  Peter Scarbrough
# Date:    7 Jan 2020
# Purpose: Demonstrate a data analysis example using principal
#          component analysis (PCA)
###
### 1) Load required packages
###
library(tidyverse)  # for data manipulation (dplyr) tools and ggplot2
library(caret)
###
### 2) Load data, do brief exploratory data anlaysis
###    Data are chemical features to try to explain critical temperature for superconductivity
###    Source: https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data
###
data <- read_csv("train.csv")
### get dimensions of dataset
### note: many columsn suggest dimension reduction would be helpful
### since all data are continuous, PCA is a good option
dim(data)
### get number of missiner per column
sapply(data, FUN = function(x) sum(is.na(x)))
### reorder columns to put dependent variable in front, everything else behind
data <- select(data, critical_temp, everything())
###
### 3) do pca of indepdendent variables, get plot of variance by PC number
###    get number of principal components to explain 95% of variance
###
depVar  <- data[,-1]
stdData <- sapply(depVar, function(x) scale(x, center=T, scale=T))
pcaData <- prcomp(stdData)
### plot cumulative variance, get 95% variance explained PC #
cumVar <- cumsum(pcaData$sdev^2)/sum(pcaData$sdev^2)
critPC <- which(cumVar > 0.95) %>% min
critPC
plot(x = 1:length(cumVar), y = cumVar,
main = "Cumulative Variance vs Principal Component",
ylab = "Proportion of Cumulative Variance",
xlab = "Principal Component",
type = "b")
abline(v = critPC, col = "red")
text(x=critPC+20, y=0.5, paste0("Min PC # for 95% Cumulative Var: ", critPC), col="red")
### select critical PC data, add critical_temp data (response variable)
critData               <- pcaData$x[,1:critPC] %>%
as.data.frame %>%
mutate(critical_temp = data$critical_temp) %>%
select(critical_temp, everything())
###
### 4) model with data post-dimensional reduction data, using random forest, summarize
###    cross validate with 10-fold cross-validation
###    note: just using default tuning grid for hyperparameter selection
###
### separate into training and test subsets
### note: since the data are so large, will not need to use cross-validation
set.seed(123)
n         <- nrow(critData)
trainI    <- sample(1:n, size=0.8*n, replace=F)
trainData <- critData[trainI,]
testData  <- critData[-trainI,]
### perform random forest modeling
### note: commenting out modeling step to save computation time
###       loading precalculated object instead
myControl <- trainControl(method="none")
# rfFit     <- train(critical_temp ~ .,
#                    data=trainData,
#                    trControl=myControl,
#                    method="rf")
### save `rfFit` so it doesn't have to be re-calculated
# save(rfFit, file="rfFit.robject")
load("rfFit.robject")
###
### 5) report model performance with root mean squared error (RMSE)
###
rfPredict <- predict(rfFit, newdata=select(testDat, -critical_temp))
rmse      <- mean((rfPredict - testData$critical_temp)^2)
rmse
rfPredict <- predict(rfFit, newdata=select(testData, -critical_temp))
rmse      <- mean((rfPredict - testData$critical_temp)^2)
rmse
rmse      <- mean((rfPredict - testData$critical_temp)^2)
rmse
rSquared  <- cor(rfPredict, testData$critical_temp)^2
rSquared
