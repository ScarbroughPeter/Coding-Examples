{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title:   Python Regression Examples\n",
    "# Author:  Peter Scarbrough\n",
    "# Date:    24 Jan 2020\n",
    "# Purpose: Practice regression in Python, leading to pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load required packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
      "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
      "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
      "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
      "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
      "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  MEDV  \n",
      "0       15.3  396.90   4.98  24.0  \n",
      "1       17.8  396.90   9.14  21.6  \n",
      "2       17.8  392.83   4.03  34.7  \n",
      "3       18.7  394.63   2.94  33.4  \n",
      "4       18.7  396.90   5.33  36.2  \n",
      "..       ...     ...    ...   ...  \n",
      "501     21.0  391.99   9.67  22.4  \n",
      "502     21.0  396.90   9.08  20.6  \n",
      "503     21.0  396.90   5.64  23.9  \n",
      "504     21.0  393.45   6.48  22.0  \n",
      "505     21.0  396.90   7.88  11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. Load, tidy data (boston housing)\n",
    "bdata = datasets.load_boston()\n",
    "\n",
    "# convert to pandas dataframe\n",
    "b_df = pd.DataFrame(bdata.data, columns=bdata.feature_names)\n",
    "b_df[\"MEDV\"] = bdata.target\n",
    "\n",
    "# print data frame to inspect\n",
    "# note: skipping EDA for this exercise\n",
    "#       but pandas df usually a good starting point for that\n",
    "print(b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "# 3. linear regression modeling - include all predictors\n",
    "#    note: this model is overfitted\n",
    "\n",
    "# get features, response variable\n",
    "X = b_df.iloc[:,:13]\n",
    "y = b_df[\"MEDV\"]\n",
    "\n",
    "# use sklearn.linear_model.LinearRegression\n",
    "lr    = LinearRegression()\n",
    "model = lr.fit(X, y)\n",
    "\n",
    "# get model R2\n",
    "r2 = model.score(X, y)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6628192101128285\n"
     ]
    }
   ],
   "source": [
    "# 4. Linear Regression w/Regularization - Lasso Method\n",
    "\n",
    "# scale X (mean = 0, std = 1)\n",
    "scaler = StandardScaler()\n",
    "stdX   = scaler.fit_transform(X)\n",
    "\n",
    "# do lasso regression\n",
    "lr_lasso    = Lasso(alpha=1)\n",
    "model_lasso = lr_lasso.fit(stdX, y) \n",
    "\n",
    "# get model R2\n",
    "r2_lasso = model_lasso.score(stdX, y)\n",
    "print(r2_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for lasso model:  0.02807216203941177\n"
     ]
    }
   ],
   "source": [
    "# 5. optimize lasso method (also a kind of feature selection)\n",
    "\n",
    "# split into training and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "\n",
    "# create a pipeline for model selection \n",
    "# include feature selection (alpha) and standardization (required for lasso)\n",
    "pipeline      = Pipeline([(\"standardize\", StandardScaler()),\n",
    "                          (\"classifier\", Lasso())])\n",
    "alpha_space   = np.logspace(-3, 3, 30)\n",
    "hyperp        = [{\"classifier__alpha\": alpha_space}]\n",
    "lasso_modeler = GridSearchCV(pipeline, hyperp, cv=10, verbose=0, iid=False)\n",
    "lasso_fit     = lasso_modeler.fit(X_train, y_train)\n",
    "\n",
    "# get optimal alpha estimate from test data using lasso modeling object\n",
    "optimal_alpha = lasso_fit.best_params_['classifier__alpha']\n",
    "print(\"Optimal alpha for lasso model: \", optimal_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated R2 from optimized lasso model:  0.7104744913539625\n"
     ]
    }
   ],
   "source": [
    "# 6. refit model with test data, calculate R2\n",
    "\n",
    "# standardize predictor (X) test data\n",
    "scaler = StandardScaler()\n",
    "std_X_test = scaler.fit(X_test).transform(X_test)\n",
    "\n",
    "# make lasso fit using the putative 'optimal_alpha' value\n",
    "lasso_fit_test = Lasso(alpha=optimal_alpha).fit(std_X_test, y_test)\n",
    "\n",
    "# get R^2 estimate from test data modeling\n",
    "lasso_fit_test_r2 = lasso_fit_test.score(std_X_test, y_test)\n",
    "print(\"Estimated R2 from optimized lasso model: \", lasso_fit_test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso model coefficients: \n",
      "[-0.5903414   0.94604396 -0.00668398  0.3508278  -2.1576922   1.07793228\n",
      "  0.52763615 -3.74778731  3.24525877 -2.36980758 -1.83938538  1.87885042\n",
      " -4.59068475]\n",
      "\n",
      "best model information: \n",
      "Ridge(alpha=13.738237958832638, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# 7. Improve pipeline, add ridge regression\n",
    "#    test lasso and ridge while tuning hyperparameters with standardization\n",
    "#    also include option to allow parallelization of calculation\n",
    "\n",
    "# note: feature selection didn't work with lasso optimization\n",
    "#       lack of zeroes suggests no automatic feature selection\n",
    "print(\"lasso model coefficients: \", lasso_fit_test.coef_, \"\", sep=\"\\n\") \n",
    "\n",
    "# so it's worth trying ridge regression as well. let's incorporate into pipeline...\n",
    "\n",
    "# split into training and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)\n",
    "\n",
    "# create a pipeline for model selection \n",
    "# include feature selection (alpha) and standardization (required for lasso)\n",
    "pipeline      = Pipeline([(\"standardize\", StandardScaler()),\n",
    "                          (\"classifier\", Ridge())])\n",
    "alpha_space    = np.logspace(-3, 3, 30)\n",
    "hyperp         = [{\"classifier\": [Lasso()],\n",
    "                   \"classifier__alpha\": alpha_space},\n",
    "                  {\"classifier\": [Ridge()],\n",
    "                   \"classifier__alpha\": alpha_space}]\n",
    "linear_modeler = GridSearchCV(pipeline, hyperp, cv=10, verbose=0, iid=False, n_jobs=-1)\n",
    "linear_fit     = linear_modeler.fit(X_train, y_train)\n",
    "\n",
    "# get best model information\n",
    "print(\"best model information: \", linear_fit.best_params_['classifier'], sep=\"\\n\")\n",
    "\n",
    "# suggests the Ridge model may perform slightly better\n",
    "optimal_alpha = linear_fit.best_params_['classifier__alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated R2 from optimized lasso model:  0.7560225184329137\n"
     ]
    }
   ],
   "source": [
    "# 8. estimate R2 from final pipeline product\n",
    "\n",
    "# standardize predictor (X) test data\n",
    "scaler = StandardScaler()\n",
    "std_X_test = scaler.fit(X_test).transform(X_test)\n",
    "\n",
    "# make lasso fit using the putative 'optimal_alpha' value\n",
    "linear_fit_test = Ridge(alpha=optimal_alpha).fit(std_X_test, y_test)\n",
    "\n",
    "# get R^2 estimate from test data modeling\n",
    "linear_fit_test_r2 = linear_fit_test.score(std_X_test, y_test)\n",
    "print(\"Estimated R2 from optimized lasso model: \", linear_fit_test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. final comments\n",
    "\n",
    "# regularization improved the R2 estimate from even the overfitted \n",
    "# original naive linear model that included all predictors\n",
    "\n",
    "# this is a promising first step in linear modeling, however repeated\n",
    "# runs suggested this estimate is unstable, with the alpha levels and\n",
    "# ridge/lasso model selection varying from run to run\n",
    "\n",
    "# this suggests it may be worth trying other machine learning models\n",
    "# playing around with feature engineering or adjusting some of the\n",
    "# cross-validation parameters in order to achieve a more stable result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
