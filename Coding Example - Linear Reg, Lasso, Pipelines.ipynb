{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title:   Python Regression Examples\n",
    "# Author:  Peter Scarbrough\n",
    "# Date:    25 Jan 2020\n",
    "# Purpose: Practice regression in Python, leading to pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load required packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
      "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
      "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
      "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
      "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
      "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  MEDV  \n",
      "0       15.3  396.90   4.98  24.0  \n",
      "1       17.8  396.90   9.14  21.6  \n",
      "2       17.8  392.83   4.03  34.7  \n",
      "3       18.7  394.63   2.94  33.4  \n",
      "4       18.7  396.90   5.33  36.2  \n",
      "..       ...     ...    ...   ...  \n",
      "501     21.0  391.99   9.67  22.4  \n",
      "502     21.0  396.90   9.08  20.6  \n",
      "503     21.0  396.90   5.64  23.9  \n",
      "504     21.0  393.45   6.48  22.0  \n",
      "505     21.0  396.90   7.88  11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2. Load, tidy data (boston housing)\n",
    "bdata = datasets.load_boston()\n",
    "\n",
    "# convert to pandas dataframe\n",
    "b_df = pd.DataFrame(bdata.data, columns=bdata.feature_names)\n",
    "b_df[\"MEDV\"] = bdata.target\n",
    "\n",
    "# print data frame to inspect\n",
    "# note: skipping EDA for this exercise\n",
    "#       but pandas df usually a good starting point for that\n",
    "print(b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 from overfit linear model:  0.7406426641094095\n"
     ]
    }
   ],
   "source": [
    "# 3. linear regression modeling - include all predictors\n",
    "#    note: this model is overfitted\n",
    "\n",
    "# get features, response variable\n",
    "X = b_df.iloc[:,:13]\n",
    "y = b_df[\"MEDV\"]\n",
    "\n",
    "# use sklearn.linear_model.LinearRegression\n",
    "lr    = LinearRegression()\n",
    "model = lr.fit(X, y)\n",
    "\n",
    "# get model R2\n",
    "r2 = model.score(X, y)\n",
    "print(\"R2 from overfit linear model: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:   \n",
    "Lack of proper model selection strategies make this R2 an overestimate of the model's generalizability. Including all predictor terms in linear regression likely resulted in an overfit model. However, there is still value in taking a rough look at this statistic because 1) it demonstrates the process of performing a simple linear regresison in Python and 2) it gives us a rough idea of the amount of information content in the data that can be used to model the response.  \n",
    "\n",
    "One way to avoid overfitting the data is to introduce regularization. Lasso is a method that will do this in linear regression while also potentially performing a kind of automated feature selection. This is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 from naive lasso model:  0.6628192101128285\n"
     ]
    }
   ],
   "source": [
    "# 4. Linear Regression w/Regularization - Lasso Method\n",
    "\n",
    "# scale X (mean = 0, std = 1)\n",
    "scaler = StandardScaler()\n",
    "stdX   = scaler.fit_transform(X)\n",
    "\n",
    "# do lasso regression\n",
    "lr_lasso    = Lasso(alpha=1)\n",
    "model_lasso = lr_lasso.fit(stdX, y) \n",
    "\n",
    "# get model R2\n",
    "r2_lasso = model_lasso.score(stdX, y)\n",
    "print(\"R2 from naive lasso model: \", r2_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments :  \n",
    "Although the score appears lower than the previous linear regression model, it is actually probably an improvement. However, to further improve the model's generalizability it is important to introduce proper model selection controls.\n",
    "\n",
    "Splitting the data into test and training subsets, and performing model selection on the training subset and final model testing on the test subset should help reduce selection bias and improve the R2 estimate and better reflect the true generalizability of the model. Additionally, the lasso method has a hyperparameter (alpha) that controls the amount of shrinkage in the model. By using cross-validation one can select an optimal hyperparameter and further improve the generalizable performance of the resulting lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for lasso model:  0.02807216203941177\n",
      "Optimal Lasso R2 estimate:  0.6094753889003006\n"
     ]
    }
   ],
   "source": [
    "# 5. optimize lasso method (also a kind of feature selection)\n",
    "\n",
    "# split into training and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "\n",
    "# create a pipeline for model selection \n",
    "# include feature selection (alpha) and standardization (required for lasso)\n",
    "pipeline      = Pipeline([(\"standardize\", StandardScaler()),\n",
    "                          (\"classifier\", Lasso())])\n",
    "alpha_space   = np.logspace(-3, 3, 30)\n",
    "hyperp        = [{\"classifier__alpha\": alpha_space}]\n",
    "lasso_modeler = GridSearchCV(pipeline, hyperp, cv=10, verbose=0, iid=False)\n",
    "lasso_fit     = lasso_modeler.fit(X_train, y_train)\n",
    "\n",
    "# get optimal alpha estimate from test data using lasso modeling object\n",
    "optimal_alpha = lasso_fit.best_params_['classifier__alpha']\n",
    "print(\"Optimal alpha for lasso model: \", optimal_alpha)\n",
    "\n",
    "# then use the optimal alpha to build the final model\n",
    "# use the test data to test that model and get an R2 estimate\n",
    "lasso_modeler = Lasso(alpha=optimal_alpha)\n",
    "lasso_fit     = lasso_modeler.fit(X_train, y_train)\n",
    "lasso_fit_r2  = lasso_fit.score(X_test, y_test)\n",
    "print(\"Optimal Lasso R2 estimate: \", lasso_fit_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:  \n",
    "Although this is the lowest reported R2 estimate yet, due to proper model selection controls being utilized, this is probably the most generalizable and believable estimate yet produced. There is probably still room for improvement. And we can do this, still working within a regression framework, by considering ridge regression as an alternative candidate model.\n",
    "\n",
    "This will be explored, below, using a pipeline to control the standardization of the data, hyperparamter tuning, and model selection all in one system. The resulting model will then be evaluated with the test data to estimate R2. Note: GridSearchCV is used to handle the cross-validation system in Python for hyperparameter tuning. Once the hyperparameter is found the final model is refit using the regular method with the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso model coefficients: \n",
      "[-1.02536033e-01  5.16042317e-02 -5.22340921e-03  2.46836096e+00\n",
      " -7.05449850e+00  4.58532400e+00 -1.41512085e-02 -1.18642856e+00\n",
      "  2.34439987e-01 -1.19731189e-02 -8.86444294e-01  8.74788492e-03\n",
      " -4.59145631e-01]\n",
      "\n",
      "Best model information: \n",
      "Ridge(alpha=13.738237958832638, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "Optimal Ridge R2 Estimate:  0.7265246392769191\n"
     ]
    }
   ],
   "source": [
    "# 6. Improve pipeline, add ridge regression as candidate model\n",
    "#    test lasso and ridge while tuning hyperparameters with standardization\n",
    "#    also include option to allow parallelization of calculation\n",
    "\n",
    "# note: feature selection didn't work with lasso optimization\n",
    "#       lack of zeroes suggests no automatic feature selection\n",
    "print(\"lasso model coefficients: \", lasso_fit.coef_, \"\", sep=\"\\n\") \n",
    "\n",
    "# so it's worth trying ridge regression as well. let's incorporate into pipeline...\n",
    "\n",
    "# split into training and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)\n",
    "\n",
    "# create a pipeline for model selection \n",
    "# include feature selection (alpha) and standardization (required for lasso)\n",
    "pipeline      = Pipeline([(\"standardize\", StandardScaler()),\n",
    "                          (\"classifier\", Ridge())])\n",
    "alpha_space    = np.logspace(-3, 3, 30)\n",
    "hyperp         = [{\"classifier\": [Lasso()],\n",
    "                   \"classifier__alpha\": alpha_space},\n",
    "                  {\"classifier\": [Ridge()],\n",
    "                   \"classifier__alpha\": alpha_space}]\n",
    "linear_modeler = GridSearchCV(pipeline, hyperp, cv=10, verbose=0, iid=False, n_jobs=-1)\n",
    "linear_fit     = linear_modeler.fit(X_train, y_train)\n",
    "\n",
    "# get best model information\n",
    "print(\"Best model information: \", linear_fit.best_params_['classifier'], \"\", sep=\"\\n\")\n",
    "\n",
    "# get optimal alpha estimate for ridge model\n",
    "optimal_alpha_ridge = linear_fit.best_params_['classifier__alpha']\n",
    "\n",
    "# then use the optimal alpha to build the final model\n",
    "# use the test data to test that model and get an R2 estimate\n",
    "ridge_modeler = Ridge(alpha=optimal_alpha_ridge)\n",
    "ridge_fit     = ridge_modeler.fit(X_train, y_train)\n",
    "ridge_fit_r2  = ridge_fit.score(X_test, y_test)\n",
    "print(\"Optimal Ridge R2 Estimate: \", ridge_fit_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "In this example, we find that ridge regression was better at preventing model overfit than the lasso method. The final R2 is an improvement upon earlier models while performing adequate model selection controls; thus it is a believable and good first step towards developing a good machine learning approach to this prediction problem.\n",
    "\n",
    "In general for these two shrinkage methods (lasso and ridge), lasso is generally preferred for larger datasets and in cases where feature selection is desired, since it has the capacity to naturally eliminate candidate features. Lasso also tends to perform better in cases where there may be more outliers that are causes overfitting of the data. However, in simpler cases, it is not uncommon to see ridge regression outperform, as it tends to be generally better than lasso at reducing model overfit. Therefore, the result we found here wasn't too surprising. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
